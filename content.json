{"pages":[],"posts":[{"title":"write config test","text":"高级设置（1）模板设置当我们使用命令 hexo new &quot;title&quot; 创建文章时，Hexo 会根据 /scaffolds/post.md 对新文章进行初始化。 换言之，/scaffolds/post.md 就是新文章的 模板，所以我们可以修改它来适应自己的写作习惯。 一个简单的示例如下： 1234title: {{ title }}date: {{ date }}tags: categories: （2）头部设置在每篇利用 Hexo 创建的文章的开头，都会有对文章进行说明的文字，叫做 文章头部 文章的头部除了可以设置文章标题、发布日期等基础信息外，还可以为文章添加标签、分类等 一个简单的示例如下： 1234title: Titledate: YYYY-MM-DD HH:MM:SStags: [tag1, tag2, ...]categories: category 注意：属性和属性值之间必须有一个空格，否则会解析错误 （3）首页显示在利用 Hexo 框架搭建的博客网站中，首页会显示文章的内容，且默认显示文章的全部内容 如果当文章太长的时候就会显得十分冗余，所以我们有必要对其进行精简 这时，我们只需在文章中使用 &lt;!--more--&gt; 标志即可，表示只会显示标志前面的内容。","link":"/2021/11/01/label-test/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/11/01/hello-world-%E5%89%AF%E6%9C%AC/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/11/01/hello-world/"},{"title":"","text":"Deep learning on mobile devices: A review(移动设备上的深度学习—综述)移动设备上实现深度学习的优点包括通信带宽低，云计算资源成本小，响应时间短，数据私密性好。 深度学习和推理在移动设备上的优势有四个方面:1)节省通信带宽。在移动设备上完成的计算越多，发送到云端的数据就越少。2)降低云计算资源成本。对于某些应用程序来说，维护甚至租用云计算资源的成本可能令人望而却步。随着移动设备的计算能力越来越强，移动设备上的计算成为可能。3)响应时间快。如果所有计算都能在本地执行，那么就不会有通信时间的开销，也不会有服务器可靠性的问题。对于某些应用程序，例如医疗保健和军事领域，此响应时间非常关键。4)移动计算将感知数据保存在本地设备上，大大提高了用户数据的隐私性。对于家庭机器人应用来说尤其如此。 1.移动深度学习硬件架构1.CPU 使用高度优化的线性代数库LAPACK/BLAS，大多数现有的深度学习框架已经考虑到这一点[1]; 充分利用多核多线程cpu进行并行计算; 利用CPU厂商专用的数学库; 对模型大小、数值精度、计算速度和系统性能进行权衡研究 2.GPUs and Mobile GPUs ​ GPU在物理设计上是一种高度并行的处理器。它通常包含数千个重复的小核，这使得在矩阵上执行某些类型的重复计算比一般用途的cpu效率更高。 3.FPGA ​ CPU是用于非常通用的计算，ASIC几乎是专门用于特定的应用，而FPGA则介于两者之间。fpga可以被“固件”(重新)编程，以非常高效地执行许多特定的应用程序。它在系统开发时间和功耗方面也处于中间位置。 4.ASIC和TPU ​ ASIC在为应用设计和制造特定芯片方面比FPGA更进一步。它有潜力实现更好的SWAP(尺寸、质量与功耗)，但代价是长期的开发时间。许多模拟、数字、混合信号、忆阻和尖峰电路被设计来模拟听觉、视觉和大脑功能。有许多asic是为人工智能应用开发的。​ TPU是谷歌专门为神经网络机器学习开发的。它被使用在最近的AlphaGo对李世石的人机围棋比赛中。虽然没有商业可用，但TPU提供给机器学习研究人员免费试用，并可通过其云计算服务访问。 5.移动深度学习的CPU、GPU、FPGA、ASIC比较 ​ 如前所述，CPU和GPU是通用计算平台，因此提供了最大的灵活性。深度学习应用的早期算法性能研究应该利用cpu和gpu来初步了解可达到的性能。cpu和gpu支持全精度计算，可以提供计算密集型模型，这通常意味着可以实现更高的预测精度。但是，gpu和cpu的能效较低。asic可以更节能，因为硬件是专门为特定的计算而设计的。然而，ASIC芯片的设计和开发可能非常耗时。因此，只有当算法研究已经确定，系统的功耗预算非常低时，才使用专用集成电路。fpga提供了功耗、预测精度和系统开发速度之间的折中。 2.移动深度学习平台和库2.1利用云人工智能服务构建移动深度学习解决方案​ 如果云上的深度学习适合应用程序，那么在移动设备上部署深度学习能力的最快方法就是利用许多现有的云人工智能api。在这种情况下，移动设备充当传感器和用户界面。这些api在机器学习、语音识别、计算机视觉、自然语言处理、人工智能助手、知识发现、个性和情感分析以及搜索等方面提供了随时可用的人工智能能力。​ 一些深度学习应用程序，或者至少是推理部分，需要在移动设备上实现，而不依赖于云机器学习。许多深度学习平台仍可用于移动应用程序。以下是一些最著名的移动深度学习平台: TensorFlow Lite来自谷歌:TensorFlow是谷歌Brain团队开发的一个开源工具，用于深度学习模型的训练和部署。其灵活的架构允许部署到各种平台，从CPU, GPU, TPU，移动和边缘设备。对于初学者来说，Keras是一个简单的python API，可以使用TensorFlow来构建模型和运行评估。TensorFlow Lite是在移动和嵌入式设备上运行TensorFlow模型的解决方案。它经过了精确、低延迟、小模型尺寸和可移植到Android、iOS和其他物联网(IoT)设备的优化。 来自Facebook的Caffe2:Caffe2是一个轻量级的、模块化的、可扩展的深度学习框架。它提供跨平台库，用于在云或移动设备上部署。Caffe模型可以小于1MB的二进制大小，并且是为了提高速度而构建的。支持ARM CPU, iPhone GPU, Android GPU。此外，它还支持导入使用CNTK和PyTorch训练的模型。 Core ML for IOS: Core machine learning是苹果公司为IOS 11开发的。它建立在低级原语之上:加速基本神经网络子程序(BNNS)，金属性能着色器(MPS)自动最小化内存占用和功耗。它直接支持Keras, Caffe, scikit -learn, XGGoost和LibSVM机器学习工具[Keras, Caffe, scikit, XGGoost, LibSVM]。此外，使用Caffe和TensorFlow模型构建的模型可以在几行代码中转换为CoreML模型。 高通骁龙神经处理SDK:它被设计用于运行在高通骁龙移动平台上的Caffe，Caffe2,ONNX，或TensorFlow训练的DNN模型。它能自动识别出最佳的目标核进行推理，无论是CPU、GPU还是DSP,如今大约有一半的安卓手机使用了Snapdragon芯片。 DeepLearningKit for Apple Devices: DeepLearningKit是一个DL框架，适用于电视(TV)、iOS (iPhone和iPad)和OS X (MacBook和iMac)。它支持CNN在Caffe训练的模型。它运行在移动GPU上。它是快速的，但自2016年以来就没有维持过。 MACE (Mobile AI Compute Engine): MACE是针对移动异构计算平台优化的深度学习推理框架。它不仅支持广泛的模型形成，如TensorFlow、Caffe和ONNx，而且还具有保护模型的技术，如将模型转换为c++代码。它优化了运行时、内存使用、库占用和UI响应。 Paddle-Mobile: Paddle-Mobile是一个更大的深度学习项目PaddlePaddle的一部分，专注于嵌入式平台。支持ARM CPU、Mali GPU、Android GPU/GPU、iOS、基于fpga的开发板、Raspberry Pi等ARM- linux开发板。 支持IOS移动系统的：TensorFlow 、Caffe2、CoreML、DeepLearningKit 支持Android移动系统的：TensorFlow、Caffe2、Snapdragon 2.2移动深度学习库 Keras。 一些深度机器学习工具包，如TensorFlow，要求用户具有非常好的机器学习和软件工程知识。同时具备这两种技能的用户非常有限，因此ML的采用非常缓慢。Keras的想法是极大地简化DL的学习和部署。通过设计一个易于使用的高级API并隐藏所有不必要的机器学习和软件细节，Keras python允许用户使用TensorFlow、CNTK或Theano，而无需通过学习曲线来详细研究每个底层库。Keras代码和文档可以在https://keras.io/上找到。Keras项目非常成功，现在已经完全集成到TensorFlow中了。Keras是最受初学者和深度学习研究人员欢迎的工具之一。在github上有许多用Keras实现的开源深度学习项目。 Torch/PyTorch。PyTorch是一个基于Torch的Python开放源码ML库。Torch本身是一个基于Lua编程语言的机器学习和科学计算的开源工具。虽然Torch已不再积极开发，但PyTorch在学术研究人员中非常受欢迎。还有许多基于PyTorch设计的开源项目和人工智能应用程序。Pytorch可在https://pytorch.org/上找到。 MXNet。MXNet来自Apache软件基金会。MXNet是一个灵活、高效的深度学习库。它将所有代码打包在一个源文件中。它的优势在于跨平台(iOS和android)，易于移植，并且适用于大多数主要编程语言。仅支持CPU部署。MXNet的网址是https://mxnet.incubator.apache.org。 CNNDroid。CNNDroid是一个开源库，用于加速CNN在Android设备上的执行。GPU加速cnn for Android支持Caffe、Torch和Theano模型，在AlexNet上使用GPU模型比CPU加速30~40倍。CNNDroid可以在https://github.com/ENCP/CNNdroid上找到。 JaveScript Libraries 。javascript是基于web应用程序的核心编程语言之一。有相当多的JavaScript库是专门为在web浏览器中进行深度学习开发而设计的，包括ConvNetJS、DeepLern.js、Keras.js、TensorFlow.js、Brain.js等。不需要安装很多库或驱动程序。这些库对于构建移动深度学习演示非常方便。这也使得移动机器学习更具互动性。其中一些只用于DNN推断，而其他的也具有训练DNN的能力。有些可以使用CPU进行训练和推断，如ConvNetJs。其他的可以使用GPU进行训练和推理，如DeepLearn.js。js在支持GPU的浏览器中运行Keras模型。对于在现有平台和库中拥有足够经验的开发者来说，开发一个移动深度学习原型可能需要几周或几个月的时间。下一步是在字段中测试应用程序。实际的测试可能会发现计算、存储、内存占用或SWAP方面需要进一步改进。特别是，当系统收集更多特定领域的数据时，您可能能够提高深度学习系统的性能。ML设计者需要对深度学习算法和软件有更深入的了解，才能将其性能提升到更高的水平。 ​ 每一组深度机器学习库和工具都有自己的优点、缺点和用例。使用这些工具训练的模型都有自己的格式。为了充分利用所有这些工具和库，能够将模型格式从一种转换为另一种可能是有益的。因此，为此目的开发了一些项目。最强大的模型转换工具之一是MMdnn。该工具可以将TensorFlow、CNTK、Keras、Caffe、PyTorch、MXNet和CoreML格式模型转换为中间表示，然后可以将中间表示转换为这些原生目标格式[28]之一。 3.移动深度学习优化在软件、算法、软硬件联合设计等方面，优化移动深度学习应用的SWAP性能。 3.1 DeepX软件加速器​ DeepX就是一个软件加速器，专为移动设备[43]上的低功耗深度学习推理而设计。该算法由两种用于深度学习推理的资源控制算法组成。一种算法将DNN体系结构分解成各种类型的块，这些块在异构的本地GPU和CPU处理器上更有效地执行。另一种算法通过资源缩放来调整DNN模型结构，从而调整每个块的开销。 3.2 SWAP的算法优化​ 深入研究机器学习神经网络的最初动机是为了获得更好的表征学习。这些学习的表示捕获了特征空间中复杂的非线性嵌入，使模型具有更好的准确性和泛化性能。这些模型通常是深度的，包含数百万个参数，通常不适合移动深度学习部署。最近，人们做了一些非常有趣的工作，使这些高精度模型更适合于移动应用程序，几乎或不牺牲准确性。下面一些方法总结了其中的一些思想，包括dnn的量化、剪枝、压缩和近似。 3.2.1 CNN架构优化​ 最近的ResNet使数百层以上的DNN训练取得了令人印象深刻的准确性。然而，精度性能并不与层数成线性关系。对于移动应用程序，在选择架构以平衡准确性和其他SWAP考虑方面可以做出很好的权衡。Iandola的SqueezeNet工作提出了三种策略来优化CNN，使其参数更少[4]: 将大部分3x3滤波器替换为1x1滤波器。 减少滤波器的输入通道数量 在网络的后期层中向下采样 ​ 在ImageNet任务上，只需50倍的参数，SqueezeNet就能达到AlexNet级别的精度。通过附加的模型压缩技术(详见下文小节)，SqueezeNet实现了0.5MB大小，比AlexNet小500倍以上。 3.3.2修剪，量化和霍夫曼编码​ Han的“深度压缩”工作采用了剪枝、量化和Huffman编码技术，在不影响精度[44]的情况下，将模型大小减少了35~49倍。修剪基本上删除了所有小权重连接。通常，使用剪枝可以将权重参数减少10倍。接下来的步骤是量化和重量共享。其思想是将所有具有相似值的权值聚在一起，并将这些权值存储在一个码本中。最后一步应用众所周知的Huffman编码将更短的代码分配给更常见的符号。​ 对于移动应用程序来说，将精度从32位降低到16位或更低也是一种常见的做法。本文对不动点近似、动态不动点近似、小浮点近似和无乘子算法等不同的近似方法进行了比较。一个名为Ristretto的近似框架是用开源代码提出的。TensorFlow还支持8位量化。 3.2.3网络二值化和xnor网络量化的一种极端情况是二值化，即权重减少到-1或+1。卷积计算基本上就是求和和减法。在XNOR网络中，滤波器和卷积层的输入都是二进制[46]。与传统的32位分辨率计算相比，这种方法节省了32倍的内存，提高了58倍的卷积运算速度。在Image-Net分类上进行评价时，二元权重网络版本的AlexNet获得了与全精度Alex-Net相同的精度。作者还提供了代码。 3.3软硬件协同设计上一小节展示了移动深度学习SWAP改进的算法进步的巨大前景。采用算法和硬件协同设计的方法制作用于移动深度学习的ASIC芯片，还可以实现更大的改进。例如，Han、Liu等人最近的ASIC协同设计工作，在能效方面分别比CPU和GPU提高了4个和3个数量级。在九个DNN基准测试[47]上，它的速度比CPU和GPU分别提高了两个和一个数量级。 3.4使用特定领域数据改进移动深度学习系统性能对于深度机器学习来说，数据可能和算法一样重要。花在收集正确数据上的努力比花在改进算法上的努力更有回报，这种情况并不少见。下面的小节给出了如何处理特定领域数据的一些指导。 虽然有大量公开可用的数据集，但移动深度学习应用可能在不同的环境下运行，使用不同的传感器，或有不同的规格。从移动深度学习应用程序将要部署的环境中收集一些真实的数据总是一个好主意。这些数据对于改进模型或验证应用程序性能是非常宝贵的。然而，使用新数据从头构建模型并不总是最好的方法。对于移动深度学习来说，对环境的控制较少，因此测试数据的可变性较大。这意味着要有一个性能良好的模型，可能需要一个庞大的训练数据集。不幸的是，数据收集和数据标记可能非常耗时。考虑到所有这些因素，采取的典型步骤是: 1.搜索与应用程序相似的预先训练过的模型。对于计算机视觉、语音识别和自然语言处理等典型的人工智能应用，已经有许多高质量的模型可供公开使用。现有AI应用模型的详细列表请参见4.4节。2.对特定领域数据的预训练模型进行微调。3.利用现有框架来部署应用程序。 利用现有的模型和框架可以节省大量的工作。然而，决定预训练模型的哪个部分需要调整是一件非常困难的事情。这取决于所获取的领域特定数据的数量，以及该数据与用于构建预训练模型的原始数据之间的相似性。下表给出了一般的指导方针。 ​ 如何利用现有的域外模型提高域数据性能的一般指导 领域数据大小 与预先训练的模型数据相似 如何提升 大 高 微调N/最后一层 小 高 微调可能会过度拟合模型，在(N-1)层激活上训练线性分类器 小 低 在低层激活时训练线性分类器，高层更具体于原始数据 大 低 使用特定领域的数据从头开始训练DNN 4.面向从业者的移动深度学习资源4.1用于移动深度学习的计算机语言实现移动深度学习的不同方法需要不同的平台、工具和编程语言。这些语言提供了不同的开发效率和实际计算效率。因此，在不同的发展阶段可以采取不同的方法。此外，要在不同的硬件平台上部署已开发的系统，还需要额外的计算机语言技能。例如，Android和iOS平台使用不同的计算机语言。 4.2移动设备上的深度学习的开放源代码库移动深度学习领域的准入门槛相对较低，因为大多数研究人员都在网上免费提供他们的源代码。最受欢迎的托管源代码的网站是github。在其他情况下，机器学习社区会提供基于不同平台或库的相同算法的多个版本。在这种情况下，比较评分以及算法的文档和维护情况通常是一个好主意。 4.3深度学习模型动物园如前一节所述，有许多公开的深度学习模型。这些模型为开发移动深度学习应用程序提供了一个良好的起点。下面列出了一些模型动物园。 TensorFlow / Keras模型动物园。这可以说是最大的动物园模型。它包含两个类别，一个用于TensorFlow官方模型，另一个用于TensorFlow研究模型。官方模型使用TensorFlow的高级API，并且倾向于更好地维护和测试，以跟上TensorFlow的最新更新。官方的模型非常宽泛。例如，官方模型的Keras版本包含以下DNN架构:Xception、Vgg、ResNet、InceptionResNet、MobileNet、DenseNet和NASNet。研究模型是使用TensorFlow实现的一些最新的dnn。例如:注意力模型、生成对手模型、深度演讲和变换器。它们是由个别研究人员维护的。模型链接是https://github.com/TensorFlow/models 微软的认知工具。该工具包提供了60多个模型，涵盖了语音、图像、文本、金融、时间序列和强化学习等应用领域。链接是https://www.microsoft.com/enus/cognitive-toolkit/features/model-gallery/ Caffe/Caffe2模型动物园。Caffe预训练模型涵盖了图像处理和计算机视觉领域最具影响力的几十个模型，如CNN、VGG、ResNet等。链接是https://github.com/BVLC/caffe/wiki/Model-Zoo和https://github.com/caffe2/models/ 动物园MXNet模型。除了一些最流行的模型也涵盖其他动物园，这个动物园包含网络中的网络模型，单镜头检测模型，LocationNet，电影评级模型，和视频游戏模拟器。链接是https://mxnet.incubator.apache.org/model_zoo/index.html 4.4移动深度学习平台性能基准(IOS、Android)现代智能手机能够提供超过100亿次的浮点运算能力。移动深度学习的基准应该考虑准确性、模型大小和速度/执行时间等指标。一个基准是比较iPhone 7上用于图像分类的各种深度学习模型。下总结了性能。 ​ 基准图像分类模型性能iPhone 7 模型架构 Top1 Acc (%) 模型大小(MB) 执行时间(ms) vgg16 71 553 208 inception v3 78 95 90 ResNet 50 75 103 64 mobilenet 71 17 32 sqeezenet 57 5 24 第二个是Android智能手机上的4个芯片组(高通、HiSillicon、MediaTck和三星)的人工智能基准测试。本文将深度学习中的8个任务作为人工智能基准进行了比较，包括图像分类、人脸识别、去模糊、超分辨率、分割和增强。这个基准测试从超过10,000个移动设备和超过50个不同的移动soc中获得了结果。详情请见http://aibenchmark.com/。其中一些结论如下: 在Android上使用深度学习最简单的方法是使用TensorFlow Mobile框架。 TensorFlow Lite是一个选项，但我们推荐用于比图像分类更复杂的任务。 对于特定的设备或SoC，可以使用专有SDK，但不那么容易和方便。 Caffe 2和其他框架没有那么流行，几乎没有教程和问题描述。 模型量化的适用性有限，性能可能不可靠。 5.移动深度学习应用5.1机器人随着人工智能技术的发展，机器人在不久的将来将扮演许多科学家和工程师所设想的角色。除了前面提到的语音、自然语言处理和计算机视觉任务外，机器人还面临一些学习、推理和实施的特定应用任务。一些最新的技术综述和关于其潜力和局限性的讨论是可以得到的。要想取得进展，一个关键技术是让机器人能够自主地从感官数据中获得技能。机器人需要能够在没有特定指令的情况下完成一般任务。这也被称为学会学习。 5.2自动驾驶自动驾驶是移动深度学习的另一个非常重要的应用，有望在不久的将来成为现实。关于深度学习在自动驾驶中的应用，最近发表了许多有趣的论文。CNN对高速公路行驶的经验评价显示了其对地面和车辆的实时检测能力。提出了一个基于统一架构的多网络，用于联合分类、检测和语义分割的实时求解。自动驾驶的障碍除了道德和法律等非技术问题外，似乎还包括在极端条件下驾驶。 5.3医疗虽然医疗资源可能有限，但智能手机设备在全球日益普及。将深度学习应用于改善医疗保健服务，可能会对福祉和经济产生深远影响。拥有如此多的内置传感器，智能手机可以被用作“医疗”设备，监测个人的身体和心理状态。例如，在对话中使用音频和文本可以检测抑郁。此外，摄像机和扬声器可以作为眼睛和声音，为视力和语言障碍的人。 5.4生物识别技术与模式识别的许多其他领域的问题类似，深度学习是许多生物识别模式的领先解决方案，包括人脸、声音、按键、指纹、手指静脉、虹膜和手势识别。由于智能手机包含如此多的用户隐私数据，安全性非常重要。大多数智能手机都有内置的指纹或面部识别生物识别技术。我们可以预见，对于未来的可共享自动驾驶汽车，出于安全考虑，移动生物识别组件也应该是必须的。在一定程度上，生物特征识别是合作对象近距离协作的一个亟待解决的问题。未解决的关键问题之一是远距离不合作的主体。 5.5个人辅助、移动多媒体、增强现实和娱乐个人辅助、移动多媒体和人的互动可以说是移动深度学习技术应用最广泛的领域。最近的一份报告显示，超过4700万美国成年人已经拥有智能音箱，更不用说智能手机上的个人辅助功能了。人工智能在未来能够生成文本、图像，甚至视频。娱乐内容交付、用户粘性和增强现实都可以通过AI技术得到改善。 5.6防御随着人工智能技术的发展，它为国防工业打开了应用的大门。半自主无人机可能会使用一些类似于自动驾驶的技术来开发。许多计算机视觉技术可用于自动目标检测、识别和跟踪。深度学习适用于电子战感知的全光谱。大多数为民用机器人开发的技术可能适用于国防领域，用于军用物资的运输和其他任务。不管技术准备水平如何，科学家、工程师、媒体和政治家在推广人工智能用于自动武器时都应该非常小心。许多道德、法律、监管和政策问题尚未解决。 6.移动深度学习的挑战和未来的工作6.1用于移动设备的自动深度机器学习如前所述，移动设备应用程序的深度学习必须针对SWAP进行优化。深度学习系统中有许多可以优化的层次参数。这些被称为超参数。超参数优化是一个被称为自动机器学习的活跃课题。一些自动ML技术可以应用到移动应用中。移动DL的目标函数不仅仅是精度，还可以是精度、内存占用、计算速度和功耗的加权函数。 6.2低质量数据和ML用于信号处理移动应用程序通常意味着应用程序的约束更少。与在实验室中收集的控制良好的数据不同，移动数据往往带有更多混杂因素，更加嘈杂。这也意味着一些(如果不是很大一部分的话)标记的数据可能是错误的。虽然DNN在训练数据较大时对噪声标签具有鲁棒性，但如果训练数据较少，则会出现问题。因此，移动深度学习工程师需要更仔细地准备用于模型训练的数据。有时可能需要自定义信号处理来处理某些特殊现象。 6.3训练数据少，样本学习少，迁移学习虽然未标记的数据可能更丰富，但移动应用程序通常一开始就拥有较少的训练数据。为了启动系统，可以使用少量样本学习或迁移学习。随着可用数据的增加，初始模型可以迭代地改进。 6.4环境的改变、在线适应和终身学习移动深度学习的另一个更具挑战性的问题是，环境可能会随着时间而改变。在一些极端情况下，系统可能不得不面对一些不可预见的情况。这些情况要求移动系统能够在线适应数据的变化或能够终身学习。 6.5移动深度学习中的隐私问题由于一些移动深度学习直接处理用户的个人数据，隐私是一个问题。如果数据是在云中存储和处理的，这一点尤其正确。最近提出了一种混合深度学习架构，用于保护隐私的移动分析。","link":"/2021/11/01/notes-for-review1-%E5%89%AF%E6%9C%AC-2/"},{"title":"","text":"Deep Learning on Mobile and Embedded Devices:State-of-the-art, Challenges, and Future Directions(移动和嵌入式设备上的深度学习:最新技术、挑战和未来方向)​ 我们首先简要介绍了深度学习，并讨论了在移动和嵌入式设备上实现深度学习模型的主要挑战。​ 然后，我们对帮助深度学习模型适应移动和嵌入式设备的重要压缩和加速技术进行了深入调查，我们具体将其分为修剪、量化、模型蒸馏、网络设计策略和低秩因子分解。本文详细阐述了基于硬件的解决方案，包括移动GPU、FPGA、ASIC，并描述了用于移动深度学习模型的软件框架，特别是基于OpenCL和RenderScript的框架开发。​ 随后，我们介绍了移动深度学习在导航、健康、语音识别、信息安全等多个领域的应用。最后，我们讨论了移动和嵌入式设备上的深度学习的一些未来方向，以启发该领域的进一步研究 1.介绍​ 移动深度学习主要有两种模式，即在线模式和离线模式。早期在移动应用中利用深度学习模型的工作主要集中在在线模式，即云执行训练和推理任务，而移动设备由于电池、计算能力和存储能力有限，只能从云发送和接收数据。但是，这种在线模式依赖于互联网连接，可能会导致很长时间的延迟。此外，将深度学习任务外包给远程服务器可能会带来用户敏感数据的隐私问题。而在离线模式下，训练任务仍然由云执行，但将训练后的模型发送到移动设备进行本地推理(边推理)，以保护用户隐私。然而，训练后的深度模型参数多，计算复杂，对有限的移动设备资源构成了巨大的挑战。​ 与在线模式相比，离线模式可以更好地保护用户隐私，大大降低响应时间、通信成本和云负担。脱机模式具有可用性、可靠性、安全性和低延迟等优点，是各种应用尤其是实时应用的首选模式。​ 不幸的是，移动和嵌入式设备的计算能力、内存、内存带宽和电池都太有限，无法支持既需要计算又需要内存的现代深度学习模型。在电池驱动的移动设备上进行深度学习模型的过度能耗也是一个亟待解决的严重问题。此外，移动和嵌入式设备的不同计算环境给移动深度学习带来了额外的挑战。 在本文中，我们对移动和嵌入式设备上的深度学习的最新研究进行了全面的综述。从三个方面提出了现有的解决方案:压缩和加速技术、硬件和软件框架。本文的组织如下: 第2节中，我们简要介绍了深度学习，并讨论了在移动和嵌入式设备上部署深度学习模型的挑战。 第3节对移动深度学习模型的压缩和加速技术进行了全面的综述。 4节概述了硬件解决方案。 第5节中，描述了移动深度学习模型的软件框架。 第6节中，我们介绍了深度学习在移动和嵌入式设备上的应用。 第7节和第8节分别讨论了隐私问题和潜在的未来方向。 第9节结束这篇文章。 本文调查了2000-2018年期间移动和嵌入式设备上的深度学习的文献。 2.背景简要介绍深度学习，并介绍在移动设备上部署深度学习的主要挑战。 2.1 深度学习深度学习分为两个阶段:训练阶段和推理阶段。在训练阶段使用训练数据训练学习模型，在推理阶段使用训练模型预测输入数据的结果。 深度学习体系结构取得了巨大的进展，其中最流行的是深度神经网络(DNN)、卷积神经网络(CNN)和递归神经网络(RNN)。CNN的典型模型有LeNet、AlexNet、VGG、GoogleNet和ResNet。递归神经网络(RNN)在许多NLP和音频处理任务中显示了巨大的前景，它使用了基于节点之间连接的有向图的顺序信息。长短期记忆(LSTM)模型由遗忘门、输入门和输出门组成，是一种有用的、常用的RNN。 2.2 挑战在移动和嵌入式设备上部署深度学习存在三大挑战。 不同的计算环境。移动和嵌入式设备的硬件和软件环境与计算机非常不同。因此，目前为计算机开发的深度学习模型不能直接应用于移动和嵌入式设备。例如，许多基于cuda的GPU加速库在空间和性能都比传统桌面GPU小的移动GPU中是不可用的。 有限的资源。dnn固有的密集计算将消耗大量资源，包括电池功率、内存和计算单元，这些资源在移动设备上是有限的。目前，如下表所示，一个标准的DNN包含数十个卷积层，这需要相当大的存储空间和计算能力，无论是在训练阶段还是推理阶段，移动和嵌入式设备都无法承受。此外，运行深度学习算法的高能耗也给电池驱动的移动设备带来了巨大挑战。 长时间的推迟。由于深度学习模型结构复杂，移动设备的计算能力有限，其推理阶段的延迟时间可能较长，可能无法满足自动驾驶汽车等实时应用的需求。 ​ 表2 网络压缩与加速技术综述 技术 描述 关键问题 特点 剪枝 移除低突出性参数 评价指标 应用广泛，性能好 量化 减少参数表示的位数 精度降低应用 应用广泛，硬件友好 模型蒸馏 训练一个精炼的学生模型，模仿一个更大的教师网络 转移知识的清晰度 从头开始训练，在NLP中很流行 网络设计策略 设计低成本和高效的架构 适当的策略 从头开始训练，几乎专门为了CNN 低秩分解 使用近似低秩的张量 分解方法 在小过滤器中不流行 3.网络压缩和加速技术网络压缩和加速技术已经引起了工业界和学术界的广泛关注。我们主要将这些方法分为7类:剪枝、量化、模型精馏、网络设计策略、低秩因子分解等技术以及混合技术。 修剪技术通过评估冗余参数对模型性能的贡献来消除冗余参数。不同的指标仍然是剪枝技术的一个重大挑战，如表2所示，已经被提出来评估参数的重要性。 量化技术使用更少的位来表示参数(例如，二进制/三元)，这有助于减少内存开销和计算时间。 模型蒸馏技术是基于学生-教师模型，它从一个更广泛的教师网络转移知识，训练一个更小的蒸馏学生模型。 网络设计策略探索设计特殊块以获得低成本架构的有效策略。 低秩因子分解技术使用张量分解来压缩和加速网络。此外，这些技术与其他技术是正交的，可以集成取得显著的性能。我们在表2中简要总结了这些网络压缩和加速技术。 3.1剪枝剪枝是一种非常流行的模型压缩和加速技术，它通过剪枝不重要和低效的参数来压缩网络。典型的剪枝算法可以分为三个阶段:评估参数的重要性，剪枝不重要的参数，微调恢复精度。 LeCun等人首先证明了一些不显著的权值可以从预训练的网络中剔除而不影响准确性。但是，这种非结构化的剪枝技术可能会导致结构不规则，不能直接加速，并且非结构化的随机连接可能会导致缓存和内存访问问题。为了解决这些限制，提出了结构化剪枝来获得规则的网络连接。因此，我们将修剪技术分为两类:非结构化修剪和结构化修剪。如表3所示，我们从技术、数据集、模型、压缩率、加速率和精度下降等方面比较了现有的不同剪枝技术。 3.1.1非结构化的修剪具体的过程看word。 4.硬件深度学习模型在移动设备上的应用对各种硬件资源的需求很大。传统移动设备的硬件资源无法满足深度学习模型的计算需求。因此，研究人员一直致力于改进各种硬件，使深度学习任务能够在移动端执行。 4.1硬件解决方案5.软件框架6.应用由于页面限制，移动深度学习的应用背景可作为在线补充材料。 7.讨论:移动深度学习的隐私问题8.未来方向 自适应/自动压缩。目前的网络压缩和加速技术涉及到很多超参数，如剪枝阈值、线性量化位宽等，需要通过经验实验和专家知识来确定，这给移动应用开发者带来了很大的压力。此外，通过微调对网络进行再训练是非常关键的，这需要专家知识和大量的实验。拥有自适应/自动的压缩和加速方法和工具是一个很有前途的方向，不依赖于手工设计，可以被开发人员轻易使用。据我们所知，一些探索已经开始[66]。 无人监督的压缩。在压缩过程中，需要标记数据对网络进行再训练，以确保数据的准确性。标签数据在现实世界中有时是不可用的，比如医学图像。此外，现实中大量的未标记数据使无监督学习变得越来越重要。无监督压缩技术是解决这些困难的理想方法。 硬件技术的发展。许多新的硬件技术，如HMC、HBM和eDRAM，已经在台式机和服务器上使用，但由于成本高，尚未在移动设备上使用。预计这些新的内存架构将在不久的将来用于移动设备。一些移动芯片已经发布，但尚未达到批量生产的规模。应该在价格和性能之间作出更多的努力。推动这些技术在移动设备上的应用，推动深度学习模型在移动设备上的部署。此外，分解、剪枝和数据量化经常用于使模型硬件友好，但可能会影响性能。应该提出更多的硬件解决方案来解决这个问题。 内存访问优化。内存访问是非常消耗能量的，而节能是移动和嵌入式设备的关键问题。解决这个问题的方法之一是将数据处理放在内存附近。存储器处理和传感器处理等方法在一定程度上提高了能源效率，但还不够。我们仍然需要设计更智能的数据流架构，以提高数据重用，并充分利用并行性，以满足移动设备对更复杂的深度学习模型日益增长的需求，而不消耗太多的能源。 创新的网络体系结构。Liu等论证了有效的网络结构对网络压缩和加速至关重要。有很大的设计空间来探索更好但低成本的网络架构。可以开发新的网络架构，如DenseNet和CliqueNet，以在较少复杂计算的情况下获得更好的性能。随着该领域的快速发展，预计在不久的将来会开发出越来越多的创新深度学习模型。此外，自动搜索网络架构来压缩和加速深度学习模型可能比手工设计网络架构更有前景。 优化技术的组合。现有的工作从不同的方面解决了移动学习适应移动设备的问题，如算法，硬件或软件。通过联合利用各种技术，移动深度学习的性能可能会进一步提高。有一些开创性的作品试图统一不同的解决方案。跨堆栈优化被用来达到更好的性能，但由于存在大量不同的压缩和加速技术、软件框架和硬件后端，并且缺乏对不同解决方案的全面概述，因此在跨堆栈优化上进行手动调优可能会令人望而却步。因此，开发人员需要自动跨堆栈优化，以便自动确定最佳的解决方案组合。例如，XLA[52]和TVM编译器被设计成在高级框架和低级硬件之间架起桥梁。","link":"/2021/11/01/notes-for-review2-%E5%89%AF%E6%9C%AC-2/"},{"title":"","text":"Deep learning on mobile devices: A review(移动设备上的深度学习—综述)移动设备上实现深度学习的优点包括通信带宽低，云计算资源成本小，响应时间短，数据私密性好。 深度学习和推理在移动设备上的优势有四个方面:1)节省通信带宽。在移动设备上完成的计算越多，发送到云端的数据就越少。2)降低云计算资源成本。对于某些应用程序来说，维护甚至租用云计算资源的成本可能令人望而却步。随着移动设备的计算能力越来越强，移动设备上的计算成为可能。3)响应时间快。如果所有计算都能在本地执行，那么就不会有通信时间的开销，也不会有服务器可靠性的问题。对于某些应用程序，例如医疗保健和军事领域，此响应时间非常关键。4)移动计算将感知数据保存在本地设备上，大大提高了用户数据的隐私性。对于家庭机器人应用来说尤其如此。 1.移动深度学习硬件架构1.CPU 使用高度优化的线性代数库LAPACK/BLAS，大多数现有的深度学习框架已经考虑到这一点[1]; 充分利用多核多线程cpu进行并行计算; 利用CPU厂商专用的数学库; 对模型大小、数值精度、计算速度和系统性能进行权衡研究 2.GPUs and Mobile GPUs ​ GPU在物理设计上是一种高度并行的处理器。它通常包含数千个重复的小核，这使得在矩阵上执行某些类型的重复计算比一般用途的cpu效率更高。 3.FPGA ​ CPU是用于非常通用的计算，ASIC几乎是专门用于特定的应用，而FPGA则介于两者之间。fpga可以被“固件”(重新)编程，以非常高效地执行许多特定的应用程序。它在系统开发时间和功耗方面也处于中间位置。 4.ASIC和TPU ​ ASIC在为应用设计和制造特定芯片方面比FPGA更进一步。它有潜力实现更好的SWAP(尺寸、质量与功耗)，但代价是长期的开发时间。许多模拟、数字、混合信号、忆阻和尖峰电路被设计来模拟听觉、视觉和大脑功能。有许多asic是为人工智能应用开发的。​ TPU是谷歌专门为神经网络机器学习开发的。它被使用在最近的AlphaGo对李世石的人机围棋比赛中。虽然没有商业可用，但TPU提供给机器学习研究人员免费试用，并可通过其云计算服务访问。 5.移动深度学习的CPU、GPU、FPGA、ASIC比较 ​ 如前所述，CPU和GPU是通用计算平台，因此提供了最大的灵活性。深度学习应用的早期算法性能研究应该利用cpu和gpu来初步了解可达到的性能。cpu和gpu支持全精度计算，可以提供计算密集型模型，这通常意味着可以实现更高的预测精度。但是，gpu和cpu的能效较低。asic可以更节能，因为硬件是专门为特定的计算而设计的。然而，ASIC芯片的设计和开发可能非常耗时。因此，只有当算法研究已经确定，系统的功耗预算非常低时，才使用专用集成电路。fpga提供了功耗、预测精度和系统开发速度之间的折中。 2.移动深度学习平台和库2.1利用云人工智能服务构建移动深度学习解决方案​ 如果云上的深度学习适合应用程序，那么在移动设备上部署深度学习能力的最快方法就是利用许多现有的云人工智能api。在这种情况下，移动设备充当传感器和用户界面。这些api在机器学习、语音识别、计算机视觉、自然语言处理、人工智能助手、知识发现、个性和情感分析以及搜索等方面提供了随时可用的人工智能能力。​ 一些深度学习应用程序，或者至少是推理部分，需要在移动设备上实现，而不依赖于云机器学习。许多深度学习平台仍可用于移动应用程序。以下是一些最著名的移动深度学习平台: TensorFlow Lite来自谷歌:TensorFlow是谷歌Brain团队开发的一个开源工具，用于深度学习模型的训练和部署。其灵活的架构允许部署到各种平台，从CPU, GPU, TPU，移动和边缘设备。对于初学者来说，Keras是一个简单的python API，可以使用TensorFlow来构建模型和运行评估。TensorFlow Lite是在移动和嵌入式设备上运行TensorFlow模型的解决方案。它经过了精确、低延迟、小模型尺寸和可移植到Android、iOS和其他物联网(IoT)设备的优化。 来自Facebook的Caffe2:Caffe2是一个轻量级的、模块化的、可扩展的深度学习框架。它提供跨平台库，用于在云或移动设备上部署。Caffe模型可以小于1MB的二进制大小，并且是为了提高速度而构建的。支持ARM CPU, iPhone GPU, Android GPU。此外，它还支持导入使用CNTK和PyTorch训练的模型。 Core ML for IOS: Core machine learning是苹果公司为IOS 11开发的。它建立在低级原语之上:加速基本神经网络子程序(BNNS)，金属性能着色器(MPS)自动最小化内存占用和功耗。它直接支持Keras, Caffe, scikit -learn, XGGoost和LibSVM机器学习工具[Keras, Caffe, scikit, XGGoost, LibSVM]。此外，使用Caffe和TensorFlow模型构建的模型可以在几行代码中转换为CoreML模型。 高通骁龙神经处理SDK:它被设计用于运行在高通骁龙移动平台上的Caffe，Caffe2,ONNX，或TensorFlow训练的DNN模型。它能自动识别出最佳的目标核进行推理，无论是CPU、GPU还是DSP,如今大约有一半的安卓手机使用了Snapdragon芯片。 DeepLearningKit for Apple Devices: DeepLearningKit是一个DL框架，适用于电视(TV)、iOS (iPhone和iPad)和OS X (MacBook和iMac)。它支持CNN在Caffe训练的模型。它运行在移动GPU上。它是快速的，但自2016年以来就没有维持过。 MACE (Mobile AI Compute Engine): MACE是针对移动异构计算平台优化的深度学习推理框架。它不仅支持广泛的模型形成，如TensorFlow、Caffe和ONNx，而且还具有保护模型的技术，如将模型转换为c++代码。它优化了运行时、内存使用、库占用和UI响应。 Paddle-Mobile: Paddle-Mobile是一个更大的深度学习项目PaddlePaddle的一部分，专注于嵌入式平台。支持ARM CPU、Mali GPU、Android GPU/GPU、iOS、基于fpga的开发板、Raspberry Pi等ARM- linux开发板。 支持IOS移动系统的：TensorFlow 、Caffe2、CoreML、DeepLearningKit 支持Android移动系统的：TensorFlow、Caffe2、Snapdragon 2.2移动深度学习库 Keras。 一些深度机器学习工具包，如TensorFlow，要求用户具有非常好的机器学习和软件工程知识。同时具备这两种技能的用户非常有限，因此ML的采用非常缓慢。Keras的想法是极大地简化DL的学习和部署。通过设计一个易于使用的高级API并隐藏所有不必要的机器学习和软件细节，Keras python允许用户使用TensorFlow、CNTK或Theano，而无需通过学习曲线来详细研究每个底层库。Keras代码和文档可以在https://keras.io/上找到。Keras项目非常成功，现在已经完全集成到TensorFlow中了。Keras是最受初学者和深度学习研究人员欢迎的工具之一。在github上有许多用Keras实现的开源深度学习项目。 Torch/PyTorch。PyTorch是一个基于Torch的Python开放源码ML库。Torch本身是一个基于Lua编程语言的机器学习和科学计算的开源工具。虽然Torch已不再积极开发，但PyTorch在学术研究人员中非常受欢迎。还有许多基于PyTorch设计的开源项目和人工智能应用程序。Pytorch可在https://pytorch.org/上找到。 MXNet。MXNet来自Apache软件基金会。MXNet是一个灵活、高效的深度学习库。它将所有代码打包在一个源文件中。它的优势在于跨平台(iOS和android)，易于移植，并且适用于大多数主要编程语言。仅支持CPU部署。MXNet的网址是https://mxnet.incubator.apache.org。 CNNDroid。CNNDroid是一个开源库，用于加速CNN在Android设备上的执行。GPU加速cnn for Android支持Caffe、Torch和Theano模型，在AlexNet上使用GPU模型比CPU加速30~40倍。CNNDroid可以在https://github.com/ENCP/CNNdroid上找到。 JaveScript Libraries 。javascript是基于web应用程序的核心编程语言之一。有相当多的JavaScript库是专门为在web浏览器中进行深度学习开发而设计的，包括ConvNetJS、DeepLern.js、Keras.js、TensorFlow.js、Brain.js等。不需要安装很多库或驱动程序。这些库对于构建移动深度学习演示非常方便。这也使得移动机器学习更具互动性。其中一些只用于DNN推断，而其他的也具有训练DNN的能力。有些可以使用CPU进行训练和推断，如ConvNetJs。其他的可以使用GPU进行训练和推理，如DeepLearn.js。js在支持GPU的浏览器中运行Keras模型。对于在现有平台和库中拥有足够经验的开发者来说，开发一个移动深度学习原型可能需要几周或几个月的时间。下一步是在字段中测试应用程序。实际的测试可能会发现计算、存储、内存占用或SWAP方面需要进一步改进。特别是，当系统收集更多特定领域的数据时，您可能能够提高深度学习系统的性能。ML设计者需要对深度学习算法和软件有更深入的了解，才能将其性能提升到更高的水平。 ​ 每一组深度机器学习库和工具都有自己的优点、缺点和用例。使用这些工具训练的模型都有自己的格式。为了充分利用所有这些工具和库，能够将模型格式从一种转换为另一种可能是有益的。因此，为此目的开发了一些项目。最强大的模型转换工具之一是MMdnn。该工具可以将TensorFlow、CNTK、Keras、Caffe、PyTorch、MXNet和CoreML格式模型转换为中间表示，然后可以将中间表示转换为这些原生目标格式[28]之一。 3.移动深度学习优化在软件、算法、软硬件联合设计等方面，优化移动深度学习应用的SWAP性能。 3.1 DeepX软件加速器​ DeepX就是一个软件加速器，专为移动设备[43]上的低功耗深度学习推理而设计。该算法由两种用于深度学习推理的资源控制算法组成。一种算法将DNN体系结构分解成各种类型的块，这些块在异构的本地GPU和CPU处理器上更有效地执行。另一种算法通过资源缩放来调整DNN模型结构，从而调整每个块的开销。 3.2 SWAP的算法优化​ 深入研究机器学习神经网络的最初动机是为了获得更好的表征学习。这些学习的表示捕获了特征空间中复杂的非线性嵌入，使模型具有更好的准确性和泛化性能。这些模型通常是深度的，包含数百万个参数，通常不适合移动深度学习部署。最近，人们做了一些非常有趣的工作，使这些高精度模型更适合于移动应用程序，几乎或不牺牲准确性。下面一些方法总结了其中的一些思想，包括dnn的量化、剪枝、压缩和近似。 3.2.1 CNN架构优化​ 最近的ResNet使数百层以上的DNN训练取得了令人印象深刻的准确性。然而，精度性能并不与层数成线性关系。对于移动应用程序，在选择架构以平衡准确性和其他SWAP考虑方面可以做出很好的权衡。Iandola的SqueezeNet工作提出了三种策略来优化CNN，使其参数更少[4]: 将大部分3x3滤波器替换为1x1滤波器。 减少滤波器的输入通道数量 在网络的后期层中向下采样 ​ 在ImageNet任务上，只需50倍的参数，SqueezeNet就能达到AlexNet级别的精度。通过附加的模型压缩技术(详见下文小节)，SqueezeNet实现了0.5MB大小，比AlexNet小500倍以上。 3.3.2修剪，量化和霍夫曼编码​ Han的“深度压缩”工作采用了剪枝、量化和Huffman编码技术，在不影响精度[44]的情况下，将模型大小减少了35~49倍。修剪基本上删除了所有小权重连接。通常，使用剪枝可以将权重参数减少10倍。接下来的步骤是量化和重量共享。其思想是将所有具有相似值的权值聚在一起，并将这些权值存储在一个码本中。最后一步应用众所周知的Huffman编码将更短的代码分配给更常见的符号。​ 对于移动应用程序来说，将精度从32位降低到16位或更低也是一种常见的做法。本文对不动点近似、动态不动点近似、小浮点近似和无乘子算法等不同的近似方法进行了比较。一个名为Ristretto的近似框架是用开源代码提出的。TensorFlow还支持8位量化。 3.2.3网络二值化和xnor网络量化的一种极端情况是二值化，即权重减少到-1或+1。卷积计算基本上就是求和和减法。在XNOR网络中，滤波器和卷积层的输入都是二进制[46]。与传统的32位分辨率计算相比，这种方法节省了32倍的内存，提高了58倍的卷积运算速度。在Image-Net分类上进行评价时，二元权重网络版本的AlexNet获得了与全精度Alex-Net相同的精度。作者还提供了代码。 3.3软硬件协同设计上一小节展示了移动深度学习SWAP改进的算法进步的巨大前景。采用算法和硬件协同设计的方法制作用于移动深度学习的ASIC芯片，还可以实现更大的改进。例如，Han、Liu等人最近的ASIC协同设计工作，在能效方面分别比CPU和GPU提高了4个和3个数量级。在九个DNN基准测试[47]上，它的速度比CPU和GPU分别提高了两个和一个数量级。 3.4使用特定领域数据改进移动深度学习系统性能对于深度机器学习来说，数据可能和算法一样重要。花在收集正确数据上的努力比花在改进算法上的努力更有回报，这种情况并不少见。下面的小节给出了如何处理特定领域数据的一些指导。 虽然有大量公开可用的数据集，但移动深度学习应用可能在不同的环境下运行，使用不同的传感器，或有不同的规格。从移动深度学习应用程序将要部署的环境中收集一些真实的数据总是一个好主意。这些数据对于改进模型或验证应用程序性能是非常宝贵的。然而，使用新数据从头构建模型并不总是最好的方法。对于移动深度学习来说，对环境的控制较少，因此测试数据的可变性较大。这意味着要有一个性能良好的模型，可能需要一个庞大的训练数据集。不幸的是，数据收集和数据标记可能非常耗时。考虑到所有这些因素，采取的典型步骤是: 1.搜索与应用程序相似的预先训练过的模型。对于计算机视觉、语音识别和自然语言处理等典型的人工智能应用，已经有许多高质量的模型可供公开使用。现有AI应用模型的详细列表请参见4.4节。2.对特定领域数据的预训练模型进行微调。3.利用现有框架来部署应用程序。 利用现有的模型和框架可以节省大量的工作。然而，决定预训练模型的哪个部分需要调整是一件非常困难的事情。这取决于所获取的领域特定数据的数量，以及该数据与用于构建预训练模型的原始数据之间的相似性。下表给出了一般的指导方针。 ​ 如何利用现有的域外模型提高域数据性能的一般指导 领域数据大小 与预先训练的模型数据相似 如何提升 大 高 微调N/最后一层 小 高 微调可能会过度拟合模型，在(N-1)层激活上训练线性分类器 小 低 在低层激活时训练线性分类器，高层更具体于原始数据 大 低 使用特定领域的数据从头开始训练DNN 4.面向从业者的移动深度学习资源4.1用于移动深度学习的计算机语言实现移动深度学习的不同方法需要不同的平台、工具和编程语言。这些语言提供了不同的开发效率和实际计算效率。因此，在不同的发展阶段可以采取不同的方法。此外，要在不同的硬件平台上部署已开发的系统，还需要额外的计算机语言技能。例如，Android和iOS平台使用不同的计算机语言。 4.2移动设备上的深度学习的开放源代码库移动深度学习领域的准入门槛相对较低，因为大多数研究人员都在网上免费提供他们的源代码。最受欢迎的托管源代码的网站是github。在其他情况下，机器学习社区会提供基于不同平台或库的相同算法的多个版本。在这种情况下，比较评分以及算法的文档和维护情况通常是一个好主意。 4.3深度学习模型动物园如前一节所述，有许多公开的深度学习模型。这些模型为开发移动深度学习应用程序提供了一个良好的起点。下面列出了一些模型动物园。 TensorFlow / Keras模型动物园。这可以说是最大的动物园模型。它包含两个类别，一个用于TensorFlow官方模型，另一个用于TensorFlow研究模型。官方模型使用TensorFlow的高级API，并且倾向于更好地维护和测试，以跟上TensorFlow的最新更新。官方的模型非常宽泛。例如，官方模型的Keras版本包含以下DNN架构:Xception、Vgg、ResNet、InceptionResNet、MobileNet、DenseNet和NASNet。研究模型是使用TensorFlow实现的一些最新的dnn。例如:注意力模型、生成对手模型、深度演讲和变换器。它们是由个别研究人员维护的。模型链接是https://github.com/TensorFlow/models 微软的认知工具。该工具包提供了60多个模型，涵盖了语音、图像、文本、金融、时间序列和强化学习等应用领域。链接是https://www.microsoft.com/enus/cognitive-toolkit/features/model-gallery/ Caffe/Caffe2模型动物园。Caffe预训练模型涵盖了图像处理和计算机视觉领域最具影响力的几十个模型，如CNN、VGG、ResNet等。链接是https://github.com/BVLC/caffe/wiki/Model-Zoo和https://github.com/caffe2/models/ 动物园MXNet模型。除了一些最流行的模型也涵盖其他动物园，这个动物园包含网络中的网络模型，单镜头检测模型，LocationNet，电影评级模型，和视频游戏模拟器。链接是https://mxnet.incubator.apache.org/model_zoo/index.html 4.4移动深度学习平台性能基准(IOS、Android)现代智能手机能够提供超过100亿次的浮点运算能力。移动深度学习的基准应该考虑准确性、模型大小和速度/执行时间等指标。一个基准是比较iPhone 7上用于图像分类的各种深度学习模型。下总结了性能。 ​ 基准图像分类模型性能iPhone 7 模型架构 Top1 Acc (%) 模型大小(MB) 执行时间(ms) vgg16 71 553 208 inception v3 78 95 90 ResNet 50 75 103 64 mobilenet 71 17 32 sqeezenet 57 5 24 第二个是Android智能手机上的4个芯片组(高通、HiSillicon、MediaTck和三星)的人工智能基准测试。本文将深度学习中的8个任务作为人工智能基准进行了比较，包括图像分类、人脸识别、去模糊、超分辨率、分割和增强。这个基准测试从超过10,000个移动设备和超过50个不同的移动soc中获得了结果。详情请见http://aibenchmark.com/。其中一些结论如下: 在Android上使用深度学习最简单的方法是使用TensorFlow Mobile框架。 TensorFlow Lite是一个选项，但我们推荐用于比图像分类更复杂的任务。 对于特定的设备或SoC，可以使用专有SDK，但不那么容易和方便。 Caffe 2和其他框架没有那么流行，几乎没有教程和问题描述。 模型量化的适用性有限，性能可能不可靠。 5.移动深度学习应用5.1机器人随着人工智能技术的发展，机器人在不久的将来将扮演许多科学家和工程师所设想的角色。除了前面提到的语音、自然语言处理和计算机视觉任务外，机器人还面临一些学习、推理和实施的特定应用任务。一些最新的技术综述和关于其潜力和局限性的讨论是可以得到的。要想取得进展，一个关键技术是让机器人能够自主地从感官数据中获得技能。机器人需要能够在没有特定指令的情况下完成一般任务。这也被称为学会学习。 5.2自动驾驶自动驾驶是移动深度学习的另一个非常重要的应用，有望在不久的将来成为现实。关于深度学习在自动驾驶中的应用，最近发表了许多有趣的论文。CNN对高速公路行驶的经验评价显示了其对地面和车辆的实时检测能力。提出了一个基于统一架构的多网络，用于联合分类、检测和语义分割的实时求解。自动驾驶的障碍除了道德和法律等非技术问题外，似乎还包括在极端条件下驾驶。 5.3医疗虽然医疗资源可能有限，但智能手机设备在全球日益普及。将深度学习应用于改善医疗保健服务，可能会对福祉和经济产生深远影响。拥有如此多的内置传感器，智能手机可以被用作“医疗”设备，监测个人的身体和心理状态。例如，在对话中使用音频和文本可以检测抑郁。此外，摄像机和扬声器可以作为眼睛和声音，为视力和语言障碍的人。 5.4生物识别技术与模式识别的许多其他领域的问题类似，深度学习是许多生物识别模式的领先解决方案，包括人脸、声音、按键、指纹、手指静脉、虹膜和手势识别。由于智能手机包含如此多的用户隐私数据，安全性非常重要。大多数智能手机都有内置的指纹或面部识别生物识别技术。我们可以预见，对于未来的可共享自动驾驶汽车，出于安全考虑，移动生物识别组件也应该是必须的。在一定程度上，生物特征识别是合作对象近距离协作的一个亟待解决的问题。未解决的关键问题之一是远距离不合作的主体。 5.5个人辅助、移动多媒体、增强现实和娱乐个人辅助、移动多媒体和人的互动可以说是移动深度学习技术应用最广泛的领域。最近的一份报告显示，超过4700万美国成年人已经拥有智能音箱，更不用说智能手机上的个人辅助功能了。人工智能在未来能够生成文本、图像，甚至视频。娱乐内容交付、用户粘性和增强现实都可以通过AI技术得到改善。 5.6防御随着人工智能技术的发展，它为国防工业打开了应用的大门。半自主无人机可能会使用一些类似于自动驾驶的技术来开发。许多计算机视觉技术可用于自动目标检测、识别和跟踪。深度学习适用于电子战感知的全光谱。大多数为民用机器人开发的技术可能适用于国防领域，用于军用物资的运输和其他任务。不管技术准备水平如何，科学家、工程师、媒体和政治家在推广人工智能用于自动武器时都应该非常小心。许多道德、法律、监管和政策问题尚未解决。 6.移动深度学习的挑战和未来的工作6.1用于移动设备的自动深度机器学习如前所述，移动设备应用程序的深度学习必须针对SWAP进行优化。深度学习系统中有许多可以优化的层次参数。这些被称为超参数。超参数优化是一个被称为自动机器学习的活跃课题。一些自动ML技术可以应用到移动应用中。移动DL的目标函数不仅仅是精度，还可以是精度、内存占用、计算速度和功耗的加权函数。 6.2低质量数据和ML用于信号处理移动应用程序通常意味着应用程序的约束更少。与在实验室中收集的控制良好的数据不同，移动数据往往带有更多混杂因素，更加嘈杂。这也意味着一些(如果不是很大一部分的话)标记的数据可能是错误的。虽然DNN在训练数据较大时对噪声标签具有鲁棒性，但如果训练数据较少，则会出现问题。因此，移动深度学习工程师需要更仔细地准备用于模型训练的数据。有时可能需要自定义信号处理来处理某些特殊现象。 6.3训练数据少，样本学习少，迁移学习虽然未标记的数据可能更丰富，但移动应用程序通常一开始就拥有较少的训练数据。为了启动系统，可以使用少量样本学习或迁移学习。随着可用数据的增加，初始模型可以迭代地改进。 6.4环境的改变、在线适应和终身学习移动深度学习的另一个更具挑战性的问题是，环境可能会随着时间而改变。在一些极端情况下，系统可能不得不面对一些不可预见的情况。这些情况要求移动系统能够在线适应数据的变化或能够终身学习。 6.5移动深度学习中的隐私问题由于一些移动深度学习直接处理用户的个人数据，隐私是一个问题。如果数据是在云中存储和处理的，这一点尤其正确。最近提出了一种混合深度学习架构，用于保护隐私的移动分析。","link":"/2021/11/01/notes-for-review1/"},{"title":"Deep Learning on Mobile and Embedded Devices:State-of-the-art, Challenges, and Future Directions(移动和嵌入式设备上的深度学习:最新技术、挑战和未来方向)","text":"Deep Learning on Mobile and Embedded Devices:State-of-the-art, Challenges, and Future Directions(移动和嵌入式设备上的深度学习:最新技术、挑战和未来方向)​ 我们首先简要介绍了深度学习，并讨论了在移动和嵌入式设备上实现深度学习模型的主要挑战。​ 然后，我们对帮助深度学习模型适应移动和嵌入式设备的重要压缩和加速技术进行了深入调查，我们具体将其分为修剪、量化、模型蒸馏、网络设计策略和低秩因子分解。本文详细阐述了基于硬件的解决方案，包括移动GPU、FPGA、ASIC，并描述了用于移动深度学习模型的软件框架，特别是基于OpenCL和RenderScript的框架开发。​ 随后，我们介绍了移动深度学习在导航、健康、语音识别、信息安全等多个领域的应用。最后，我们讨论了移动和嵌入式设备上的深度学习的一些未来方向，以启发该领域的进一步研究 1.介绍​ 移动深度学习主要有两种模式，即在线模式和离线模式。早期在移动应用中利用深度学习模型的工作主要集中在在线模式，即云执行训练和推理任务，而移动设备由于电池、计算能力和存储能力有限，只能从云发送和接收数据。但是，这种在线模式依赖于互联网连接，可能会导致很长时间的延迟。此外，将深度学习任务外包给远程服务器可能会带来用户敏感数据的隐私问题。而在离线模式下，训练任务仍然由云执行，但将训练后的模型发送到移动设备进行本地推理(边推理)，以保护用户隐私。然而，训练后的深度模型参数多，计算复杂，对有限的移动设备资源构成了巨大的挑战。​ 与在线模式相比，离线模式可以更好地保护用户隐私，大大降低响应时间、通信成本和云负担。脱机模式具有可用性、可靠性、安全性和低延迟等优点，是各种应用尤其是实时应用的首选模式。​ 不幸的是，移动和嵌入式设备的计算能力、内存、内存带宽和电池都太有限，无法支持既需要计算又需要内存的现代深度学习模型。在电池驱动的移动设备上进行深度学习模型的过度能耗也是一个亟待解决的严重问题。此外，移动和嵌入式设备的不同计算环境给移动深度学习带来了额外的挑战。 在本文中，我们对移动和嵌入式设备上的深度学习的最新研究进行了全面的综述。从三个方面提出了现有的解决方案:压缩和加速技术、硬件和软件框架。本文的组织如下: 第2节中，我们简要介绍了深度学习，并讨论了在移动和嵌入式设备上部署深度学习模型的挑战。 第3节对移动深度学习模型的压缩和加速技术进行了全面的综述。 4节概述了硬件解决方案。 第5节中，描述了移动深度学习模型的软件框架。 第6节中，我们介绍了深度学习在移动和嵌入式设备上的应用。 第7节和第8节分别讨论了隐私问题和潜在的未来方向。 第9节结束这篇文章。 本文调查了2000-2018年期间移动和嵌入式设备上的深度学习的文献。 2.背景简要介绍深度学习，并介绍在移动设备上部署深度学习的主要挑战。 2.1 深度学习深度学习分为两个阶段:训练阶段和推理阶段。在训练阶段使用训练数据训练学习模型，在推理阶段使用训练模型预测输入数据的结果。 深度学习体系结构取得了巨大的进展，其中最流行的是深度神经网络(DNN)、卷积神经网络(CNN)和递归神经网络(RNN)。CNN的典型模型有LeNet、AlexNet、VGG、GoogleNet和ResNet。递归神经网络(RNN)在许多NLP和音频处理任务中显示了巨大的前景，它使用了基于节点之间连接的有向图的顺序信息。长短期记忆(LSTM)模型由遗忘门、输入门和输出门组成，是一种有用的、常用的RNN。 2.2 挑战在移动和嵌入式设备上部署深度学习存在三大挑战。 不同的计算环境。移动和嵌入式设备的硬件和软件环境与计算机非常不同。因此，目前为计算机开发的深度学习模型不能直接应用于移动和嵌入式设备。例如，许多基于cuda的GPU加速库在空间和性能都比传统桌面GPU小的移动GPU中是不可用的。 有限的资源。dnn固有的密集计算将消耗大量资源，包括电池功率、内存和计算单元，这些资源在移动设备上是有限的。目前，如下表所示，一个标准的DNN包含数十个卷积层，这需要相当大的存储空间和计算能力，无论是在训练阶段还是推理阶段，移动和嵌入式设备都无法承受。此外，运行深度学习算法的高能耗也给电池驱动的移动设备带来了巨大挑战。 长时间的推迟。由于深度学习模型结构复杂，移动设备的计算能力有限，其推理阶段的延迟时间可能较长，可能无法满足自动驾驶汽车等实时应用的需求。 ​ 表2 网络压缩与加速技术综述 技术 描述 关键问题 特点 剪枝 移除低突出性参数 评价指标 应用广泛，性能好 量化 减少参数表示的位数 精度降低应用 应用广泛，硬件友好 模型蒸馏 训练一个精炼的学生模型，模仿一个更大的教师网络 转移知识的清晰度 从头开始训练，在NLP中很流行 网络设计策略 设计低成本和高效的架构 适当的策略 从头开始训练，几乎专门为了CNN 低秩分解 使用近似低秩的张量 分解方法 在小过滤器中不流行 3.网络压缩和加速技术网络压缩和加速技术已经引起了工业界和学术界的广泛关注。我们主要将这些方法分为7类:剪枝、量化、模型精馏、网络设计策略、低秩因子分解等技术以及混合技术。 修剪技术通过评估冗余参数对模型性能的贡献来消除冗余参数。不同的指标仍然是剪枝技术的一个重大挑战，如表2所示，已经被提出来评估参数的重要性。 量化技术使用更少的位来表示参数(例如，二进制/三元)，这有助于减少内存开销和计算时间。 模型蒸馏技术是基于学生-教师模型，它从一个更广泛的教师网络转移知识，训练一个更小的蒸馏学生模型。 网络设计策略探索设计特殊块以获得低成本架构的有效策略。 低秩因子分解技术使用张量分解来压缩和加速网络。此外，这些技术与其他技术是正交的，可以集成取得显著的性能。我们在表2中简要总结了这些网络压缩和加速技术。 3.1剪枝剪枝是一种非常流行的模型压缩和加速技术，它通过剪枝不重要和低效的参数来压缩网络。典型的剪枝算法可以分为三个阶段:评估参数的重要性，剪枝不重要的参数，微调恢复精度。 LeCun等人首先证明了一些不显著的权值可以从预训练的网络中剔除而不影响准确性。但是，这种非结构化的剪枝技术可能会导致结构不规则，不能直接加速，并且非结构化的随机连接可能会导致缓存和内存访问问题。为了解决这些限制，提出了结构化剪枝来获得规则的网络连接。因此，我们将修剪技术分为两类:非结构化修剪和结构化修剪。如表3所示，我们从技术、数据集、模型、压缩率、加速率和精度下降等方面比较了现有的不同剪枝技术。 3.1.1非结构化的修剪具体的过程看word。 4.硬件深度学习模型在移动设备上的应用对各种硬件资源的需求很大。传统移动设备的硬件资源无法满足深度学习模型的计算需求。因此，研究人员一直致力于改进各种硬件，使深度学习任务能够在移动端执行。 4.1硬件解决方案5.软件框架6.应用由于页面限制，移动深度学习的应用背景可作为在线补充材料。 7.讨论:移动深度学习的隐私问题8.未来方向 自适应/自动压缩。目前的网络压缩和加速技术涉及到很多超参数，如剪枝阈值、线性量化位宽等，需要通过经验实验和专家知识来确定，这给移动应用开发者带来了很大的压力。此外，通过微调对网络进行再训练是非常关键的，这需要专家知识和大量的实验。拥有自适应/自动的压缩和加速方法和工具是一个很有前途的方向，不依赖于手工设计，可以被开发人员轻易使用。据我们所知，一些探索已经开始[66]。 无人监督的压缩。在压缩过程中，需要标记数据对网络进行再训练，以确保数据的准确性。标签数据在现实世界中有时是不可用的，比如医学图像。此外，现实中大量的未标记数据使无监督学习变得越来越重要。无监督压缩技术是解决这些困难的理想方法。 硬件技术的发展。许多新的硬件技术，如HMC、HBM和eDRAM，已经在台式机和服务器上使用，但由于成本高，尚未在移动设备上使用。预计这些新的内存架构将在不久的将来用于移动设备。一些移动芯片已经发布，但尚未达到批量生产的规模。应该在价格和性能之间作出更多的努力。推动这些技术在移动设备上的应用，推动深度学习模型在移动设备上的部署。此外，分解、剪枝和数据量化经常用于使模型硬件友好，但可能会影响性能。应该提出更多的硬件解决方案来解决这个问题。 内存访问优化。内存访问是非常消耗能量的，而节能是移动和嵌入式设备的关键问题。解决这个问题的方法之一是将数据处理放在内存附近。存储器处理和传感器处理等方法在一定程度上提高了能源效率，但还不够。我们仍然需要设计更智能的数据流架构，以提高数据重用，并充分利用并行性，以满足移动设备对更复杂的深度学习模型日益增长的需求，而不消耗太多的能源。 创新的网络体系结构。Liu等论证了有效的网络结构对网络压缩和加速至关重要。有很大的设计空间来探索更好但低成本的网络架构。可以开发新的网络架构，如DenseNet和CliqueNet，以在较少复杂计算的情况下获得更好的性能。随着该领域的快速发展，预计在不久的将来会开发出越来越多的创新深度学习模型。此外，自动搜索网络架构来压缩和加速深度学习模型可能比手工设计网络架构更有前景。 优化技术的组合。现有的工作从不同的方面解决了移动学习适应移动设备的问题，如算法，硬件或软件。通过联合利用各种技术，移动深度学习的性能可能会进一步提高。有一些开创性的作品试图统一不同的解决方案。跨堆栈优化被用来达到更好的性能，但由于存在大量不同的压缩和加速技术、软件框架和硬件后端，并且缺乏对不同解决方案的全面概述，因此在跨堆栈优化上进行手动调优可能会令人望而却步。因此，开发人员需要自动跨堆栈优化，以便自动确定最佳的解决方案组合。例如，XLA[52]和TVM编译器被设计成在高级框架和低级硬件之间架起桥梁。","link":"/2021/11/01/notes-for-review2-%E5%89%AF%E6%9C%AC-%E5%89%AF%E6%9C%AC/"},{"title":"","text":"title: Deep Learning on Mobile and Embedded Devices:State-of-the-art, Challenges, and Future Directions(移动和嵌入式设备上的深度学习:最新技术、挑战和未来方向) Deep Learning on Mobile and Embedded Devices:State-of-the-art, Challenges, and Future Directions(移动和嵌入式设备上的深度学习:最新技术、挑战和未来方向)​ 我们首先简要介绍了深度学习，并讨论了在移动和嵌入式设备上实现深度学习模型的主要挑战。​ 然后，我们对帮助深度学习模型适应移动和嵌入式设备的重要压缩和加速技术进行了深入调查，我们具体将其分为修剪、量化、模型蒸馏、网络设计策略和低秩因子分解。本文详细阐述了基于硬件的解决方案，包括移动GPU、FPGA、ASIC，并描述了用于移动深度学习模型的软件框架，特别是基于OpenCL和RenderScript的框架开发。​ 随后，我们介绍了移动深度学习在导航、健康、语音识别、信息安全等多个领域的应用。最后，我们讨论了移动和嵌入式设备上的深度学习的一些未来方向，以启发该领域的进一步研究 1.介绍​ 移动深度学习主要有两种模式，即在线模式和离线模式。早期在移动应用中利用深度学习模型的工作主要集中在在线模式，即云执行训练和推理任务，而移动设备由于电池、计算能力和存储能力有限，只能从云发送和接收数据。但是，这种在线模式依赖于互联网连接，可能会导致很长时间的延迟。此外，将深度学习任务外包给远程服务器可能会带来用户敏感数据的隐私问题。而在离线模式下，训练任务仍然由云执行，但将训练后的模型发送到移动设备进行本地推理(边推理)，以保护用户隐私。然而，训练后的深度模型参数多，计算复杂，对有限的移动设备资源构成了巨大的挑战。​ 与在线模式相比，离线模式可以更好地保护用户隐私，大大降低响应时间、通信成本和云负担。脱机模式具有可用性、可靠性、安全性和低延迟等优点，是各种应用尤其是实时应用的首选模式。​ 不幸的是，移动和嵌入式设备的计算能力、内存、内存带宽和电池都太有限，无法支持既需要计算又需要内存的现代深度学习模型。在电池驱动的移动设备上进行深度学习模型的过度能耗也是一个亟待解决的严重问题。此外，移动和嵌入式设备的不同计算环境给移动深度学习带来了额外的挑战。 在本文中，我们对移动和嵌入式设备上的深度学习的最新研究进行了全面的综述。从三个方面提出了现有的解决方案:压缩和加速技术、硬件和软件框架。本文的组织如下: 第2节中，我们简要介绍了深度学习，并讨论了在移动和嵌入式设备上部署深度学习模型的挑战。 第3节对移动深度学习模型的压缩和加速技术进行了全面的综述。 4节概述了硬件解决方案。 第5节中，描述了移动深度学习模型的软件框架。 第6节中，我们介绍了深度学习在移动和嵌入式设备上的应用。 第7节和第8节分别讨论了隐私问题和潜在的未来方向。 第9节结束这篇文章。 本文调查了2000-2018年期间移动和嵌入式设备上的深度学习的文献。 2.背景简要介绍深度学习，并介绍在移动设备上部署深度学习的主要挑战。 2.1 深度学习深度学习分为两个阶段:训练阶段和推理阶段。在训练阶段使用训练数据训练学习模型，在推理阶段使用训练模型预测输入数据的结果。 深度学习体系结构取得了巨大的进展，其中最流行的是深度神经网络(DNN)、卷积神经网络(CNN)和递归神经网络(RNN)。CNN的典型模型有LeNet、AlexNet、VGG、GoogleNet和ResNet。递归神经网络(RNN)在许多NLP和音频处理任务中显示了巨大的前景，它使用了基于节点之间连接的有向图的顺序信息。长短期记忆(LSTM)模型由遗忘门、输入门和输出门组成，是一种有用的、常用的RNN。 2.2 挑战在移动和嵌入式设备上部署深度学习存在三大挑战。 不同的计算环境。移动和嵌入式设备的硬件和软件环境与计算机非常不同。因此，目前为计算机开发的深度学习模型不能直接应用于移动和嵌入式设备。例如，许多基于cuda的GPU加速库在空间和性能都比传统桌面GPU小的移动GPU中是不可用的。 有限的资源。dnn固有的密集计算将消耗大量资源，包括电池功率、内存和计算单元，这些资源在移动设备上是有限的。目前，如下表所示，一个标准的DNN包含数十个卷积层，这需要相当大的存储空间和计算能力，无论是在训练阶段还是推理阶段，移动和嵌入式设备都无法承受。此外，运行深度学习算法的高能耗也给电池驱动的移动设备带来了巨大挑战。 长时间的推迟。由于深度学习模型结构复杂，移动设备的计算能力有限，其推理阶段的延迟时间可能较长，可能无法满足自动驾驶汽车等实时应用的需求。 ​ 表2 网络压缩与加速技术综述 技术 描述 关键问题 特点 剪枝 移除低突出性参数 评价指标 应用广泛，性能好 量化 减少参数表示的位数 精度降低应用 应用广泛，硬件友好 模型蒸馏 训练一个精炼的学生模型，模仿一个更大的教师网络 转移知识的清晰度 从头开始训练，在NLP中很流行 网络设计策略 设计低成本和高效的架构 适当的策略 从头开始训练，几乎专门为了CNN 低秩分解 使用近似低秩的张量 分解方法 在小过滤器中不流行 3.网络压缩和加速技术网络压缩和加速技术已经引起了工业界和学术界的广泛关注。我们主要将这些方法分为7类:剪枝、量化、模型精馏、网络设计策略、低秩因子分解等技术以及混合技术。 修剪技术通过评估冗余参数对模型性能的贡献来消除冗余参数。不同的指标仍然是剪枝技术的一个重大挑战，如表2所示，已经被提出来评估参数的重要性。 量化技术使用更少的位来表示参数(例如，二进制/三元)，这有助于减少内存开销和计算时间。 模型蒸馏技术是基于学生-教师模型，它从一个更广泛的教师网络转移知识，训练一个更小的蒸馏学生模型。 网络设计策略探索设计特殊块以获得低成本架构的有效策略。 低秩因子分解技术使用张量分解来压缩和加速网络。此外，这些技术与其他技术是正交的，可以集成取得显著的性能。我们在表2中简要总结了这些网络压缩和加速技术。 3.1剪枝剪枝是一种非常流行的模型压缩和加速技术，它通过剪枝不重要和低效的参数来压缩网络。典型的剪枝算法可以分为三个阶段:评估参数的重要性，剪枝不重要的参数，微调恢复精度。 LeCun等人首先证明了一些不显著的权值可以从预训练的网络中剔除而不影响准确性。但是，这种非结构化的剪枝技术可能会导致结构不规则，不能直接加速，并且非结构化的随机连接可能会导致缓存和内存访问问题。为了解决这些限制，提出了结构化剪枝来获得规则的网络连接。因此，我们将修剪技术分为两类:非结构化修剪和结构化修剪。如表3所示，我们从技术、数据集、模型、压缩率、加速率和精度下降等方面比较了现有的不同剪枝技术。 3.1.1非结构化的修剪具体的过程看word。 4.硬件深度学习模型在移动设备上的应用对各种硬件资源的需求很大。传统移动设备的硬件资源无法满足深度学习模型的计算需求。因此，研究人员一直致力于改进各种硬件，使深度学习任务能够在移动端执行。 4.1硬件解决方案5.软件框架6.应用由于页面限制，移动深度学习的应用背景可作为在线补充材料。 7.讨论:移动深度学习的隐私问题8.未来方向 自适应/自动压缩。目前的网络压缩和加速技术涉及到很多超参数，如剪枝阈值、线性量化位宽等，需要通过经验实验和专家知识来确定，这给移动应用开发者带来了很大的压力。此外，通过微调对网络进行再训练是非常关键的，这需要专家知识和大量的实验。拥有自适应/自动的压缩和加速方法和工具是一个很有前途的方向，不依赖于手工设计，可以被开发人员轻易使用。据我们所知，一些探索已经开始[66]。 无人监督的压缩。在压缩过程中，需要标记数据对网络进行再训练，以确保数据的准确性。标签数据在现实世界中有时是不可用的，比如医学图像。此外，现实中大量的未标记数据使无监督学习变得越来越重要。无监督压缩技术是解决这些困难的理想方法。 硬件技术的发展。许多新的硬件技术，如HMC、HBM和eDRAM，已经在台式机和服务器上使用，但由于成本高，尚未在移动设备上使用。预计这些新的内存架构将在不久的将来用于移动设备。一些移动芯片已经发布，但尚未达到批量生产的规模。应该在价格和性能之间作出更多的努力。推动这些技术在移动设备上的应用，推动深度学习模型在移动设备上的部署。此外，分解、剪枝和数据量化经常用于使模型硬件友好，但可能会影响性能。应该提出更多的硬件解决方案来解决这个问题。 内存访问优化。内存访问是非常消耗能量的，而节能是移动和嵌入式设备的关键问题。解决这个问题的方法之一是将数据处理放在内存附近。存储器处理和传感器处理等方法在一定程度上提高了能源效率，但还不够。我们仍然需要设计更智能的数据流架构，以提高数据重用，并充分利用并行性，以满足移动设备对更复杂的深度学习模型日益增长的需求，而不消耗太多的能源。 创新的网络体系结构。Liu等论证了有效的网络结构对网络压缩和加速至关重要。有很大的设计空间来探索更好但低成本的网络架构。可以开发新的网络架构，如DenseNet和CliqueNet，以在较少复杂计算的情况下获得更好的性能。随着该领域的快速发展，预计在不久的将来会开发出越来越多的创新深度学习模型。此外，自动搜索网络架构来压缩和加速深度学习模型可能比手工设计网络架构更有前景。 优化技术的组合。现有的工作从不同的方面解决了移动学习适应移动设备的问题，如算法，硬件或软件。通过联合利用各种技术，移动深度学习的性能可能会进一步提高。有一些开创性的作品试图统一不同的解决方案。跨堆栈优化被用来达到更好的性能，但由于存在大量不同的压缩和加速技术、软件框架和硬件后端，并且缺乏对不同解决方案的全面概述，因此在跨堆栈优化上进行手动调优可能会令人望而却步。因此，开发人员需要自动跨堆栈优化，以便自动确定最佳的解决方案组合。例如，XLA[52]和TVM编译器被设计成在高级框架和低级硬件之间架起桥梁。","link":"/2021/11/01/notes-for-review2-%E5%89%AF%E6%9C%AC/"},{"title":"","text":"Deep Learning on Mobile and Embedded Devices:State-of-the-art, Challenges, and Future Directions(移动和嵌入式设备上的深度学习:最新技术、挑战和未来方向)​ 我们首先简要介绍了深度学习，并讨论了在移动和嵌入式设备上实现深度学习模型的主要挑战。​ 然后，我们对帮助深度学习模型适应移动和嵌入式设备的重要压缩和加速技术进行了深入调查，我们具体将其分为修剪、量化、模型蒸馏、网络设计策略和低秩因子分解。本文详细阐述了基于硬件的解决方案，包括移动GPU、FPGA、ASIC，并描述了用于移动深度学习模型的软件框架，特别是基于OpenCL和RenderScript的框架开发。​ 随后，我们介绍了移动深度学习在导航、健康、语音识别、信息安全等多个领域的应用。最后，我们讨论了移动和嵌入式设备上的深度学习的一些未来方向，以启发该领域的进一步研究 1.介绍​ 移动深度学习主要有两种模式，即在线模式和离线模式。早期在移动应用中利用深度学习模型的工作主要集中在在线模式，即云执行训练和推理任务，而移动设备由于电池、计算能力和存储能力有限，只能从云发送和接收数据。但是，这种在线模式依赖于互联网连接，可能会导致很长时间的延迟。此外，将深度学习任务外包给远程服务器可能会带来用户敏感数据的隐私问题。而在离线模式下，训练任务仍然由云执行，但将训练后的模型发送到移动设备进行本地推理(边推理)，以保护用户隐私。然而，训练后的深度模型参数多，计算复杂，对有限的移动设备资源构成了巨大的挑战。​ 与在线模式相比，离线模式可以更好地保护用户隐私，大大降低响应时间、通信成本和云负担。脱机模式具有可用性、可靠性、安全性和低延迟等优点，是各种应用尤其是实时应用的首选模式。​ 不幸的是，移动和嵌入式设备的计算能力、内存、内存带宽和电池都太有限，无法支持既需要计算又需要内存的现代深度学习模型。在电池驱动的移动设备上进行深度学习模型的过度能耗也是一个亟待解决的严重问题。此外，移动和嵌入式设备的不同计算环境给移动深度学习带来了额外的挑战。 在本文中，我们对移动和嵌入式设备上的深度学习的最新研究进行了全面的综述。从三个方面提出了现有的解决方案:压缩和加速技术、硬件和软件框架。本文的组织如下: 第2节中，我们简要介绍了深度学习，并讨论了在移动和嵌入式设备上部署深度学习模型的挑战。 第3节对移动深度学习模型的压缩和加速技术进行了全面的综述。 4节概述了硬件解决方案。 第5节中，描述了移动深度学习模型的软件框架。 第6节中，我们介绍了深度学习在移动和嵌入式设备上的应用。 第7节和第8节分别讨论了隐私问题和潜在的未来方向。 第9节结束这篇文章。 本文调查了2000-2018年期间移动和嵌入式设备上的深度学习的文献。 2.背景简要介绍深度学习，并介绍在移动设备上部署深度学习的主要挑战。 2.1 深度学习深度学习分为两个阶段:训练阶段和推理阶段。在训练阶段使用训练数据训练学习模型，在推理阶段使用训练模型预测输入数据的结果。 深度学习体系结构取得了巨大的进展，其中最流行的是深度神经网络(DNN)、卷积神经网络(CNN)和递归神经网络(RNN)。CNN的典型模型有LeNet、AlexNet、VGG、GoogleNet和ResNet。递归神经网络(RNN)在许多NLP和音频处理任务中显示了巨大的前景，它使用了基于节点之间连接的有向图的顺序信息。长短期记忆(LSTM)模型由遗忘门、输入门和输出门组成，是一种有用的、常用的RNN。 2.2 挑战在移动和嵌入式设备上部署深度学习存在三大挑战。 不同的计算环境。移动和嵌入式设备的硬件和软件环境与计算机非常不同。因此，目前为计算机开发的深度学习模型不能直接应用于移动和嵌入式设备。例如，许多基于cuda的GPU加速库在空间和性能都比传统桌面GPU小的移动GPU中是不可用的。 有限的资源。dnn固有的密集计算将消耗大量资源，包括电池功率、内存和计算单元，这些资源在移动设备上是有限的。目前，如下表所示，一个标准的DNN包含数十个卷积层，这需要相当大的存储空间和计算能力，无论是在训练阶段还是推理阶段，移动和嵌入式设备都无法承受。此外，运行深度学习算法的高能耗也给电池驱动的移动设备带来了巨大挑战。 长时间的推迟。由于深度学习模型结构复杂，移动设备的计算能力有限，其推理阶段的延迟时间可能较长，可能无法满足自动驾驶汽车等实时应用的需求。 ​ 表2 网络压缩与加速技术综述 技术 描述 关键问题 特点 剪枝 移除低突出性参数 评价指标 应用广泛，性能好 量化 减少参数表示的位数 精度降低应用 应用广泛，硬件友好 模型蒸馏 训练一个精炼的学生模型，模仿一个更大的教师网络 转移知识的清晰度 从头开始训练，在NLP中很流行 网络设计策略 设计低成本和高效的架构 适当的策略 从头开始训练，几乎专门为了CNN 低秩分解 使用近似低秩的张量 分解方法 在小过滤器中不流行 3.网络压缩和加速技术网络压缩和加速技术已经引起了工业界和学术界的广泛关注。我们主要将这些方法分为7类:剪枝、量化、模型精馏、网络设计策略、低秩因子分解等技术以及混合技术。 修剪技术通过评估冗余参数对模型性能的贡献来消除冗余参数。不同的指标仍然是剪枝技术的一个重大挑战，如表2所示，已经被提出来评估参数的重要性。 量化技术使用更少的位来表示参数(例如，二进制/三元)，这有助于减少内存开销和计算时间。 模型蒸馏技术是基于学生-教师模型，它从一个更广泛的教师网络转移知识，训练一个更小的蒸馏学生模型。 网络设计策略探索设计特殊块以获得低成本架构的有效策略。 低秩因子分解技术使用张量分解来压缩和加速网络。此外，这些技术与其他技术是正交的，可以集成取得显著的性能。我们在表2中简要总结了这些网络压缩和加速技术。 3.1剪枝剪枝是一种非常流行的模型压缩和加速技术，它通过剪枝不重要和低效的参数来压缩网络。典型的剪枝算法可以分为三个阶段:评估参数的重要性，剪枝不重要的参数，微调恢复精度。 LeCun等人首先证明了一些不显著的权值可以从预训练的网络中剔除而不影响准确性。但是，这种非结构化的剪枝技术可能会导致结构不规则，不能直接加速，并且非结构化的随机连接可能会导致缓存和内存访问问题。为了解决这些限制，提出了结构化剪枝来获得规则的网络连接。因此，我们将修剪技术分为两类:非结构化修剪和结构化修剪。如表3所示，我们从技术、数据集、模型、压缩率、加速率和精度下降等方面比较了现有的不同剪枝技术。 3.1.1非结构化的修剪具体的过程看word。 4.硬件深度学习模型在移动设备上的应用对各种硬件资源的需求很大。传统移动设备的硬件资源无法满足深度学习模型的计算需求。因此，研究人员一直致力于改进各种硬件，使深度学习任务能够在移动端执行。 4.1硬件解决方案5.软件框架6.应用由于页面限制，移动深度学习的应用背景可作为在线补充材料。 7.讨论:移动深度学习的隐私问题8.未来方向 自适应/自动压缩。目前的网络压缩和加速技术涉及到很多超参数，如剪枝阈值、线性量化位宽等，需要通过经验实验和专家知识来确定，这给移动应用开发者带来了很大的压力。此外，通过微调对网络进行再训练是非常关键的，这需要专家知识和大量的实验。拥有自适应/自动的压缩和加速方法和工具是一个很有前途的方向，不依赖于手工设计，可以被开发人员轻易使用。据我们所知，一些探索已经开始[66]。 无人监督的压缩。在压缩过程中，需要标记数据对网络进行再训练，以确保数据的准确性。标签数据在现实世界中有时是不可用的，比如医学图像。此外，现实中大量的未标记数据使无监督学习变得越来越重要。无监督压缩技术是解决这些困难的理想方法。 硬件技术的发展。许多新的硬件技术，如HMC、HBM和eDRAM，已经在台式机和服务器上使用，但由于成本高，尚未在移动设备上使用。预计这些新的内存架构将在不久的将来用于移动设备。一些移动芯片已经发布，但尚未达到批量生产的规模。应该在价格和性能之间作出更多的努力。推动这些技术在移动设备上的应用，推动深度学习模型在移动设备上的部署。此外，分解、剪枝和数据量化经常用于使模型硬件友好，但可能会影响性能。应该提出更多的硬件解决方案来解决这个问题。 内存访问优化。内存访问是非常消耗能量的，而节能是移动和嵌入式设备的关键问题。解决这个问题的方法之一是将数据处理放在内存附近。存储器处理和传感器处理等方法在一定程度上提高了能源效率，但还不够。我们仍然需要设计更智能的数据流架构，以提高数据重用，并充分利用并行性，以满足移动设备对更复杂的深度学习模型日益增长的需求，而不消耗太多的能源。 创新的网络体系结构。Liu等论证了有效的网络结构对网络压缩和加速至关重要。有很大的设计空间来探索更好但低成本的网络架构。可以开发新的网络架构，如DenseNet和CliqueNet，以在较少复杂计算的情况下获得更好的性能。随着该领域的快速发展，预计在不久的将来会开发出越来越多的创新深度学习模型。此外，自动搜索网络架构来压缩和加速深度学习模型可能比手工设计网络架构更有前景。 优化技术的组合。现有的工作从不同的方面解决了移动学习适应移动设备的问题，如算法，硬件或软件。通过联合利用各种技术，移动深度学习的性能可能会进一步提高。有一些开创性的作品试图统一不同的解决方案。跨堆栈优化被用来达到更好的性能，但由于存在大量不同的压缩和加速技术、软件框架和硬件后端，并且缺乏对不同解决方案的全面概述，因此在跨堆栈优化上进行手动调优可能会令人望而却步。因此，开发人员需要自动跨堆栈优化，以便自动确定最佳的解决方案组合。例如，XLA[52]和TVM编译器被设计成在高级框架和低级硬件之间架起桥梁。","link":"/2021/11/01/notes-for-review2/"},{"title":"","text":"了解了增量学习和锚框的概念。 增量学习在传统分类任务中，为了保证训练得到的分类模型具有准确性和高可靠性，由两个基本假设:1.学习的训练样本和新的测试样本满足独立同分布2.必须有足够可用的训练样本 增量学习的能力就是能够不断地处理现实世界中连续的信息流,在吸收新知识的同时保留甚至整合 优化旧知识的能力。 造成灾难性遗忘的一个主要原因是 传统模型假设数据分布是固定或平稳的,训练样本是独立同分布的 ,所以模型可以一遍又一遍地看到所有任务相同的数据,但当数据变为连续的数据流时,训练数据的分布就是非平稳的,模型从非平稳的数据分布中持续不断地获取知识时,新知识会干扰旧知识,从而导致模型性能的快速下降,甚至完全覆盖或遗忘以前学习到的旧知识。 为了克服灾难性遗忘,我们希望模型一方面必须表现出从新数据中整合新知识和提炼已有知识的能力(可塑性),另一方面又必须防止新输入对已有知识的显著干扰(稳定性) 这两个互相冲突的需求构成了所谓的 稳定性-可塑性困境(stability-plasticity dilemma) 增量学习目前还没有一个特别清晰的定义,因此比较容易与在线学习,迁移学习和多任务学习等概念混淆, 尤其要注意增量学习和在线学习的区别,在线学习通常要求每个样本只能使用一次,且数据全都来自于同一个任务,而增量学习是多任务的,但它允许在进入下一个任务之前多次处理当前任务的数据。 增量学习主要关注的是灾难性遗忘（Catastrophic forgetting），平衡新知识与旧知识之间的关系，即如何在学习新知识的情况下不忘记旧知识。引用Robipolikar对增量学习算法的定义,即一个增量学习算法应同时具有以下特点:1） 可以从新数据中学习新知识2） 以前已经处理过的数据不需要重复处理3） 每次只有一个训练观测样本被看到和学习4） 学习新知识的同时能保持以前学习到的大部分知识5） 一旦学习完成后训练观测样本被丢弃6） 学习系统没有关于整个训练样本的先验知识 迁移学习迁移学习是指用已存有的知识对不同但相关领域问题进行求解的一种机器学习方法。这里迁移学习放宽了上面的两个基本假设，我们可以迁移已有的知识来解决目标领域中仅有少量有标签样本数据甚至没有学习样本的问题。（当然了，目标领域样本量肯定是越多越好的）。当不同元素共享的因素越多，迁移学习就越容易，就好像你学会了骑自行车，可能就很容易学会骑摩托车，但是学会骑自行车再去骑三轮车，可能就会很不适应。（亲身经历，确实如此，三轮车每次拐弯的时候，可能时因为重心的位置与自行车不一样，总有一种要翻车的感觉，很容易失去平衡）","link":"/2021/11/01/paperNote-%E5%89%AF%E6%9C%AC-2/"},{"title":"","text":"了解了增量学习和锚框的概念。 增量学习在传统分类任务中，为了保证训练得到的分类模型具有准确性和高可靠性，由两个基本假设:1.学习的训练样本和新的测试样本满足独立同分布2.必须有足够可用的训练样本 增量学习的能力就是能够不断地处理现实世界中连续的信息流,在吸收新知识的同时保留甚至整合 优化旧知识的能力。 造成灾难性遗忘的一个主要原因是 传统模型假设数据分布是固定或平稳的,训练样本是独立同分布的 ,所以模型可以一遍又一遍地看到所有任务相同的数据,但当数据变为连续的数据流时,训练数据的分布就是非平稳的,模型从非平稳的数据分布中持续不断地获取知识时,新知识会干扰旧知识,从而导致模型性能的快速下降,甚至完全覆盖或遗忘以前学习到的旧知识。 为了克服灾难性遗忘,我们希望模型一方面必须表现出从新数据中整合新知识和提炼已有知识的能力(可塑性),另一方面又必须防止新输入对已有知识的显著干扰(稳定性) 这两个互相冲突的需求构成了所谓的 稳定性-可塑性困境(stability-plasticity dilemma) 增量学习目前还没有一个特别清晰的定义,因此比较容易与在线学习,迁移学习和多任务学习等概念混淆, 尤其要注意增量学习和在线学习的区别,在线学习通常要求每个样本只能使用一次,且数据全都来自于同一个任务,而增量学习是多任务的,但它允许在进入下一个任务之前多次处理当前任务的数据。 增量学习主要关注的是灾难性遗忘（Catastrophic forgetting），平衡新知识与旧知识之间的关系，即如何在学习新知识的情况下不忘记旧知识。引用Robipolikar对增量学习算法的定义,即一个增量学习算法应同时具有以下特点:1） 可以从新数据中学习新知识2） 以前已经处理过的数据不需要重复处理3） 每次只有一个训练观测样本被看到和学习4） 学习新知识的同时能保持以前学习到的大部分知识5） 一旦学习完成后训练观测样本被丢弃6） 学习系统没有关于整个训练样本的先验知识 迁移学习迁移学习是指用已存有的知识对不同但相关领域问题进行求解的一种机器学习方法。这里迁移学习放宽了上面的两个基本假设，我们可以迁移已有的知识来解决目标领域中仅有少量有标签样本数据甚至没有学习样本的问题。（当然了，目标领域样本量肯定是越多越好的）。当不同元素共享的因素越多，迁移学习就越容易，就好像你学会了骑自行车，可能就很容易学会骑摩托车，但是学会骑自行车再去骑三轮车，可能就会很不适应。（亲身经历，确实如此，三轮车每次拐弯的时候，可能时因为重心的位置与自行车不一样，总有一种要翻车的感觉，很容易失去平衡）","link":"/2021/11/01/paperNote-%E5%89%AF%E6%9C%AC-%E5%89%AF%E6%9C%AC/"},{"title":"","text":"了解了增量学习和锚框的概念。 增量学习在传统分类任务中，为了保证训练得到的分类模型具有准确性和高可靠性，由两个基本假设:1.学习的训练样本和新的测试样本满足独立同分布2.必须有足够可用的训练样本 增量学习的能力就是能够不断地处理现实世界中连续的信息流,在吸收新知识的同时保留甚至整合 优化旧知识的能力。 造成灾难性遗忘的一个主要原因是 传统模型假设数据分布是固定或平稳的,训练样本是独立同分布的 ,所以模型可以一遍又一遍地看到所有任务相同的数据,但当数据变为连续的数据流时,训练数据的分布就是非平稳的,模型从非平稳的数据分布中持续不断地获取知识时,新知识会干扰旧知识,从而导致模型性能的快速下降,甚至完全覆盖或遗忘以前学习到的旧知识。 为了克服灾难性遗忘,我们希望模型一方面必须表现出从新数据中整合新知识和提炼已有知识的能力(可塑性),另一方面又必须防止新输入对已有知识的显著干扰(稳定性) 这两个互相冲突的需求构成了所谓的 稳定性-可塑性困境(stability-plasticity dilemma) 增量学习目前还没有一个特别清晰的定义,因此比较容易与在线学习,迁移学习和多任务学习等概念混淆, 尤其要注意增量学习和在线学习的区别,在线学习通常要求每个样本只能使用一次,且数据全都来自于同一个任务,而增量学习是多任务的,但它允许在进入下一个任务之前多次处理当前任务的数据。 增量学习主要关注的是灾难性遗忘（Catastrophic forgetting），平衡新知识与旧知识之间的关系，即如何在学习新知识的情况下不忘记旧知识。引用Robipolikar对增量学习算法的定义,即一个增量学习算法应同时具有以下特点:1） 可以从新数据中学习新知识2） 以前已经处理过的数据不需要重复处理3） 每次只有一个训练观测样本被看到和学习4） 学习新知识的同时能保持以前学习到的大部分知识5） 一旦学习完成后训练观测样本被丢弃6） 学习系统没有关于整个训练样本的先验知识 迁移学习迁移学习是指用已存有的知识对不同但相关领域问题进行求解的一种机器学习方法。这里迁移学习放宽了上面的两个基本假设，我们可以迁移已有的知识来解决目标领域中仅有少量有标签样本数据甚至没有学习样本的问题。（当然了，目标领域样本量肯定是越多越好的）。当不同元素共享的因素越多，迁移学习就越容易，就好像你学会了骑自行车，可能就很容易学会骑摩托车，但是学会骑自行车再去骑三轮车，可能就会很不适应。（亲身经历，确实如此，三轮车每次拐弯的时候，可能时因为重心的位置与自行车不一样，总有一种要翻车的感觉，很容易失去平衡）","link":"/2021/11/01/paperNote-%E5%89%AF%E6%9C%AC/"},{"title":"","text":"了解了增量学习和锚框的概念。 增量学习在传统分类任务中，为了保证训练得到的分类模型具有准确性和高可靠性，由两个基本假设:1.学习的训练样本和新的测试样本满足独立同分布2.必须有足够可用的训练样本 增量学习的能力就是能够不断地处理现实世界中连续的信息流,在吸收新知识的同时保留甚至整合 优化旧知识的能力。 造成灾难性遗忘的一个主要原因是 传统模型假设数据分布是固定或平稳的,训练样本是独立同分布的 ,所以模型可以一遍又一遍地看到所有任务相同的数据,但当数据变为连续的数据流时,训练数据的分布就是非平稳的,模型从非平稳的数据分布中持续不断地获取知识时,新知识会干扰旧知识,从而导致模型性能的快速下降,甚至完全覆盖或遗忘以前学习到的旧知识。 为了克服灾难性遗忘,我们希望模型一方面必须表现出从新数据中整合新知识和提炼已有知识的能力(可塑性),另一方面又必须防止新输入对已有知识的显著干扰(稳定性) 这两个互相冲突的需求构成了所谓的 稳定性-可塑性困境(stability-plasticity dilemma) 增量学习目前还没有一个特别清晰的定义,因此比较容易与在线学习,迁移学习和多任务学习等概念混淆, 尤其要注意增量学习和在线学习的区别,在线学习通常要求每个样本只能使用一次,且数据全都来自于同一个任务,而增量学习是多任务的,但它允许在进入下一个任务之前多次处理当前任务的数据。 增量学习主要关注的是灾难性遗忘（Catastrophic forgetting），平衡新知识与旧知识之间的关系，即如何在学习新知识的情况下不忘记旧知识。引用Robipolikar对增量学习算法的定义,即一个增量学习算法应同时具有以下特点:1） 可以从新数据中学习新知识2） 以前已经处理过的数据不需要重复处理3） 每次只有一个训练观测样本被看到和学习4） 学习新知识的同时能保持以前学习到的大部分知识5） 一旦学习完成后训练观测样本被丢弃6） 学习系统没有关于整个训练样本的先验知识 迁移学习迁移学习是指用已存有的知识对不同但相关领域问题进行求解的一种机器学习方法。这里迁移学习放宽了上面的两个基本假设，我们可以迁移已有的知识来解决目标领域中仅有少量有标签样本数据甚至没有学习样本的问题。（当然了，目标领域样本量肯定是越多越好的）。当不同元素共享的因素越多，迁移学习就越容易，就好像你学会了骑自行车，可能就很容易学会骑摩托车，但是学会骑自行车再去骑三轮车，可能就会很不适应。（亲身经历，确实如此，三轮车每次拐弯的时候，可能时因为重心的位置与自行车不一样，总有一种要翻车的感觉，很容易失去平衡）","link":"/2021/11/01/paperNote/"}],"tags":[{"name":"study","slug":"study","link":"/tags/study/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/%E5%AD%A6%E4%B9%A0/"}]}